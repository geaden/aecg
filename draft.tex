%% Article
\documentclass[runningheads, draft]{llncs}
%% Packages
\usepackage[T1]{fontenc}
%% Math
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[makeroom]{cancel}
%% References
\usepackage[english]{cleveref}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{hyphenat}
\usepackage{cite}
%% Algorithm
\usepackage{algorithm}
\usepackage{algorithmic}
%% Commands
\newcommand*{\eq}[1]
{
  \begin{equation*}
    #1
  \end{equation*}
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\vprod}[1]{\langle#1\rangle}
\newcommand{\errgrad}{\hat{g}}
\begin{document}
%
\title{The Adaptive Variant of Frank-Wolfe Method with Inexact Gradient}
%
\author{Gennady Denisov\inst{1} \and
    Fedor Stonyakin\inst{1,2}\orcidID{0000-0002-9250-4438} \and
    Mohammad Alkousa\inst{3,1}\orcidID{0000-0001-5470-0182}}
\authorrunning{F. Stonyakin et al.}
%
\institute{Moscow Institute of Physics and Technology, 9 Institutsky lane,
    Dolgoprudny, 141701, Russia. \and
    V.\,I.\,Vernadsky Crimean Federal University, 4 Academician Vernadsky Avenue,
    Simferopol, 295007, Republic of Crimea, Russia. \and
    Innopolis University, Innopolis, Russia\\
    %
    \email{denisov.ga@phystech.su, fedyor@mail.ru, m.alkousa@innopolis.ru}}
%
\maketitle
%
\begin{abstract}
    The article introduces the adaptive versions of the Erroneous Conditional
    Gradient (\textbf{ECG}) algorithm with an Erroneous Oracle (\textbf{EO})
    and a Linear Minimization Oracle (\textbf{LMO}) on a box-constrained
    feasible set. Two step-size strategies are studied: the first one displaying
    a dependency on the iteration, while the second one depends on the
    L-smoothness constant. This paper highlights the results of the
    implementation of these algorithms tested through computational
    experiments. PageRank is chosen for the algorithms to be applied to the
    optimization problem since the complexity of the former remains relevant
    even nowadays. The quality of the solution aligns with the theoretical
    expectations. Further research and practical implications of these
    algorithms are discussed in the conclusion.

    \keywords{Frank-Wolfe method \and Adaptive method \and
        Erroneous Conditional Gradient \and Inexact Gradient.}
\end{abstract}
%
\section{Introduction}
%
\begin{equation}\label{eq:optimization-problem}
    \min_{x \in \mathcal{C}} f(x)\tag{P}
\end{equation}
%
\subsection{Mathematical preliminaries}
%
If $f(\cdot)$ is smooth then if facilitates the descent lemma.
%
\begin{lemma}{the decsent lemma}. For any $x, y \in \mathcal{C}$ it holds that
    \begin{equation}\label{lemma:descent-lemma}
        f(x) - f(y) \leqslant f(y) + \vprod{\nabla f(x), y - x} +
        \frac{L}{2} \norm{y - x}^2_2
    \end{equation}
\end{lemma}
%
\begin{lemma}Suppose $f$ is convex. Let $w^* \in \mathcal{C}$ be the optimal
    solution of \eqref{eq:optimization-problem} and ${(w^t, \errgrad^t, p^t)}$
    be a sequence generated by the \textbf{ECG} algorithm. Then

    \begin{equation}\label{eq:erroneous-gradient-props}
        \vprod{\errgrad^t, p^{t+1}} \leqslant f^* - f^t + \varepsilon M R
    \end{equation}
\end{lemma}
%
\begin{proof}
    Let $w^t, w^*, d \in \mathbb{R}, \forall t \geqslant 0$. From the convexity
    of $f$, the choice $p^{t+1}$ and the definition on \textbf{EO}:

    \begin{equation}
        \begin{split}
            -\min_{d \in C}\vprod{\errgrad_t, d - w^t} \geqslant
            \vprod{\errgrad_t, w^t - w^*}              \\
            \vprod{\errgrad^t, w^t - w^*} = \vprod{\errgrad^t - g^t, w^t - w^*} +
            \vprod{g^t, w^t - w^*}                     \\
            \vprod{g^t, w^t - w^*} \geqslant f^t - f^* \\
            \norm{\errgrad^t - g^t} \leqslant \varepsilon \norm{g^t}
        \end{split}
    \end{equation}

    According to Cauchy-Bunyakovsky-Schwarz inequality:

    \begin{equation}
        \begin{split}
            \vprod{\errgrad^t - g^t, w^t - w^*} +
            \vprod{g^t, w^t - w^*}\geqslant \\
            \\\geqslant- \norm{\errgrad^t - g^t} \norm{w^t - w^*}
            + f^t - f^* \geqslant           \\
            \geqslant - \varepsilon \norm{g^t} R
            + f^t - f^* \geqslant - \varepsilon M R + f^t - f^*
        \end{split}
    \end{equation}

    Then:

    \begin{equation}
        \begin{split}
            f^t - f^* \leqslant \varepsilon M R -
            \min\limits_{d \in C}\vprod{\errgrad_t, d - w^t} \\
            f^t - f^* \leqslant \varepsilon M R - \vprod{\errgrad_t, p^{t + 1}}
        \end{split}
    \end{equation}
\end{proof}
%
\section{Iteration-dependent step-size}
%
\begin{equation}\label{eq:iteration-dependent-step-size}
    \eta_t = \frac{2}{t + 2}, \forall t \geqslant 0
\end{equation}
%
\subsection{Unbounded $L_t$}
%
\begin{lemma}\label{lemma:aecg-unbounded-lt}Let $\eta_t$ be the step-size, then:
    \begin{equation}
        L_t = \frac{2\Bigl(f^{t+1} - f^t - \eta_t\vprod{\errgrad^t,p^{t+1}}\Bigr)}
        {\eta_t^2\norm{p^{t+1}}^2}
    \end{equation}
\end{lemma}
%
\begin{algorithm}[!ht]
    \caption{Erroneous Conditional Gradient (ECG) with the adaptive unbounded
        $L_t$}
    \begin{algorithmic}[1]
        \REQUIRE $N > 0$ -- maximum iterations, $w^0 \in \mathcal{C},
            \varepsilon \in [0, 1)$\;
        \STATE Set $w^t \leftarrow w^0$\;
        \FOR{$t=0,1,\ldots,N-1$}
        \STATE retrieve $\hat{g}^t \leftarrow \mathcal{O}(\nabla f(w^t), \varepsilon)$\;
        \STATE compute $p^{t+1} \leftarrow LMO(\hat{g}^t) - w^t$
        \STATE compute $\eta_t \leftarrow \frac{2}{t+2}$\;
        \STATE set $w^{t+1} \leftarrow w^t + \eta_t p^{t+1}$\;
        \STATE compute $L_t \leftarrow \frac{2(f^{t+1} - f^t - \eta_t\vprod{\errgrad^t,p^{t+1}})}
            {\eta_t^2\norm{p^{t+1}}^2}$\;
        \ENDFOR
        \RETURN $w_{t+1}, L_t$
    \end{algorithmic}\label{alg:ecg-unbounded-lt}
\end{algorithm}
%
\begin{theorem} Let $\eta_t = \frac{2}{t + 2}$ be the step-size of the
    Algorithm \ref{alg:ecg-unbounded-lt}, then the following inequalities hold:

    \begin{equation}\label{eq:aecg-convergencre-rate-theorem-unbounded-lt}
        f^t - f^* \leqslant \varepsilon M R + \frac{4 L_t R^2}{t+2}
    \end{equation}

    \begin{equation}
        f^{t+1} - f^* \leqslant \varepsilon M R + \frac{t + 1}{(t+2)^2}4 L_t R^2
    \end{equation}
\end{theorem}
%
\begin{proof}
    From the descent lemma \eqref{lemma:descent-lemma} and
    the properties of \textbf{EO} follows that:

    \begin{equation}\label{eq:erroneous-gradient-inexact-gradient}
        f^{t+1} - f^* \leqslant f^t - f^* + \eta_t \vprod{\errgrad^t, p^{t+1}} +
        \frac{L_t \eta_t^2}{2} \norm{p^{t+1}}^2
    \end{equation}

    Plugging the step-size regime \eqref{eq:iteration-dependent-step-size} into
    \eqref{eq:erroneous-gradient-inexact-gradient}:

    \begin{equation}
        \begin{split}
            f^{t+1} - f^* \leqslant f^t - f^* + \frac{2}{t+2}
            \vprod{\errgrad^t, p^{t+1}} + \frac{4 L_t R^2}{t + 2}
        \end{split}
    \end{equation}

    By the lemma \eqref{eq:erroneous-gradient-props} follows:

    \begin{equation}\label{eq:aecg-convergence-rate-proof}
        \begin{split}
            f^{t+1} - f^* \leqslant f^t - f^* + \frac{2}{t+2}
            \Bigl(f^* - f^t + \varepsilon M R\Bigr) +
            \frac{4 L_t R^2}{t + 2} \leqslant \\
            \leqslant \frac{t}{t + 2}\Bigl(f^t - f^*\Bigr)+
            \frac{2 \varepsilon M R}{t + 2} + \frac{4 L_t R^2}{(t + 2)^2}
        \end{split}
    \end{equation}

    Using induction, for $t = 0$ follows:

    \begin{equation}
        f^1 - f^* \leqslant \varepsilon M R + \frac{4 L_t R^2}{4}
    \end{equation}

    Hence,

    \begin{equation}\label{eq:induction-assumption}
        f^t - f^* \leqslant \varepsilon M R + \frac{4 L_t R^2}{t + 2}
    \end{equation}

    From relation \eqref{eq:aecg-convergence-rate-proof} and assumption
    \eqref{eq:induction-assumption} we can show that
    \eqref{eq:aecg-convergencre-rate-theorem-unbounded-lt} holds true for $t + 1$:

    \begin{equation}\label{eq:aecg-convergence-rage-t+1}
        \begin{split}
            f^{t+1} - f^* \leqslant \frac{t}{t + 2}\Bigl(f^t - f^*\Bigr)+
            \frac{2 \varepsilon M R}{t + 2} + \frac{4 L_t R^2}{(t + 2)^2}
            \leqslant \\
            \leqslant \frac{t}{t + 2}\Bigl(\varepsilon M R +
            \frac{4 L_t R^2}{t + 2}\Bigr) +
            \frac{2 \varepsilon M R}{t + 2} + \frac{4 L_t R^2}{(t + 2)^2}
            \leqslant \\
            \leqslant \varepsilon M R + \frac{t + 1}{(t + 2)^2} 4 L_t R^2
        \end{split}
    \end{equation}

    Relation (\ref{eq:aecg-convergence-rage-t+1}) requires
    $\frac{t + 1}{(t + 2)^2} \leqslant \frac{1}{t + 3}$.

\end{proof}
%
\subsection{Bounded $L_t$}
%
\begin{lemma}\label{lemma:aecg-bounded-lt}Let $\eta_t$ be the step-size, then:
    \begin{equation}
        L_t = \frac{2\Bigl(f^{t+1} - f^t - \eta_t\vprod{\errgrad^t,p^{t+1}} -
            \eta_t \varepsilon M R\Bigr)}
        {\eta_t^2\norm{p^{t+1}}^2}
    \end{equation}
\end{lemma}
%
\begin{algorithm}[!ht]
    \caption{Erroneous Conditional Gradient (ECG) with the adaptive bounded
        $L_t$}
    \begin{algorithmic}[1]
        \REQUIRE $N > 0$ -- maximum iterations, $w^0 \in \mathcal{C},
            \varepsilon \in [0, 1), M > 0, R > 0$\;
        \STATE Set $w^t \leftarrow w^0$\;
        \FOR{$t=0,1,\ldots,N-1$}
        \STATE retrieve $\hat{g}^t \leftarrow \mathcal{O}(\nabla f(w^t), \varepsilon)$\;
        \STATE compute $p^{t+1} \leftarrow LMO(\hat{g}^t) - w^t$
        \STATE compute $\eta_t \leftarrow \frac{2}{t+2}$\;
        \STATE set $w^{t+1} \leftarrow w^t + \eta_t p^{t+1}$\;
        \STATE compute $L_t \leftarrow \frac{2(f^{t+1} - f^t -
            \eta_t\vprod{\errgrad^t,p^{t+1}} - \eta_t \varepsilon M R)}
            {\eta_t^2\norm{p^{t+1}}^2}$\;
        \ENDFOR
        \RETURN $w_{t+1}, L_t$
    \end{algorithmic}\label{alg:ecg-bounded-lt}
\end{algorithm}
%
\begin{theorem}Let $\eta_t = \frac{2}{t + 2}$ be the step-size of the
    Algorithm \ref{alg:ecg-bounded-lt} then the following inequalities hold:

    \begin{equation}\label{eq:aecg-convergence-rate-theorem-bounded-lt}
        f^t - f^* \leqslant 2 \varepsilon M R +
        \frac{4 \max\limits_{i=\overline{0,t+1}}L_i R^2}{t+2}
    \end{equation}

    \begin{equation}
        f^{t+1} - f^* \leqslant 2 \varepsilon M R +
        \frac{t + 1}{(t+2)^2}4 \max\limits_{i=\overline{0,t+1}}L_i R^2
    \end{equation}
\end{theorem}
%
\begin{proof}
    From the descent lemma \eqref{lemma:descent-lemma} and
    the properties of \textbf{EO} follows that:

    \begin{equation}\label{eq:erroneous-gradient-inexact-gradient-bounded-lt}
        f^{t+1} - f^* \leqslant f^t - f^* + \eta_t \vprod{\errgrad^t, p^{t+1}} +
        \frac{\max\limits_{i=\overline{0,t+1}}L_i \eta_t^2}{2} \norm{p^{t+1}}^2
        + \eta_t \varepsilon M R
    \end{equation}

    Plugging step-size regime \eqref{eq:iteration-dependent-step-size} into
    \eqref{eq:erroneous-gradient-inexact-gradient-bounded-lt}:

    \begin{equation}
        \begin{split}
            f^{t+1} - f^* \leqslant f^t - f^* + \frac{2}{t+2}
            \vprod{\errgrad^t, p^{t+1}} + \frac{4 L_t R^2}{t + 2} +
            \frac{2}{t + 2}\varepsilon M R
        \end{split}
    \end{equation}

    By the lemma \eqref{eq:erroneous-gradient-props} follows:

    \begin{equation}\label{eq:aecg-convergence-rate-proof-bounded-lt}
        \begin{split}
            f^{t+1} - f^* \leqslant f^t - f^* + \frac{2}{t+2}
            \Bigl(f^* - f^t + \varepsilon M R\Bigr) +
            \frac{2}{t + 2}\varepsilon M R +
            \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{t + 2} \leqslant \\
            \leqslant \frac{t}{t + 2}\Bigl(f^t - f^*\Bigr)+
            \frac{4}{t + 2}\varepsilon M R +
            \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{(t + 2)^2}
        \end{split}
    \end{equation}

    By induction, for $t = 0$ follows:

    \begin{equation}
        f^1 - f^* \leqslant 2\varepsilon M R +
        \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{4}
    \end{equation}

    Hence,

    \begin{equation}\label{eq:induction-assumption-bounded-lt}
        f^t - f^* \leqslant 2\varepsilon M R +
        \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{t + 2}
    \end{equation}

    From relation \eqref{eq:aecg-convergence-rate-proof-bounded-lt} and
    assumption \eqref{eq:induction-assumption-bounded-lt} we can show that
    \eqref{eq:aecg-convergence-rate-theorem-bounded-lt} holds true for $t + 1$:

    \begin{equation}\label{eq:aecg-convergence-rage-t+1-bounded-lt}
        \begin{split}
            f^{t+1} - f^* \leqslant \frac{t}{t + 2}\Bigl(f^t - f^*\Bigr)+
            \frac{4}{t + 2} \varepsilon M R +
            \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{(t + 2)^2}
            \leqslant \\
            \leqslant \frac{t}{t + 2}\Bigl(2 \varepsilon M R +
            \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{t + 2}\Bigr) +
            \frac{4}{t + 2}\varepsilon M R +
            \frac{4 \max\limits_{i=\overline{0,t}}L_i R^2}{(t + 2)^2}
            \leqslant \\
            \leqslant \frac{2t + 4}{t + 2}\varepsilon M R +
            \frac{t + 1}{(t + 2)^2} 4 \max\limits_{i=\overline{0,t}}L_i R^2
            \leqslant \\
            \leqslant 2\varepsilon M R +
            \frac{t + 1}{(t + 2)^2} 4 \max\limits_{i=\overline{0,t}}L_i R^2
        \end{split}
    \end{equation}

    Relation (\ref{eq:aecg-convergence-rage-t+1-bounded-lt}) requires
    $\frac{t + 1}{(t + 2)^2} \leqslant \frac{1}{t + 3}$.

\end{proof}
%
\section{L-smoothness constant-dependent step-size}
%
\begin{equation}\label{eq:step-size-l-smoothness-constant-dependent}
    \eta_t = -\frac{\vprod{\errgrad^t, p^{t+1}}}{L_t \norm{p^{t+1}}^2}
\end{equation}
%
\subsection{Unbounded $L_t$}
%
\begin{algorithm}[!ht]
    \caption{Adaptive Erroneous Conditional Gradient (AECG) with unbounded $L_t$}
    \begin{algorithmic}[1]
        \REQUIRE $N > 0$ -- maximum iterations, $w^0 \in \mathcal{C}, L_0 > 0, \varepsilon \in [0, 1)$.
        \STATE Set $w^t \leftarrow w^0$\;
        \STATE Set $L_{t-1} \leftarrow L_0$\;
        \FOR{$t=0,1,\ldots,N-1$}
        \STATE set $L_{t} \leftarrow \frac{L_{t-1}}{2}$\;
        \STATE retrieve $\hat{g}^t \leftarrow \mathcal{O}(\nabla f(w^t), \varepsilon)$\;
        \STATE compute $p^{t+1} \leftarrow LMO(\hat{g}^t) - w^t$
        \STATE compute $\eta_t \leftarrow -\frac{\vprod{\errgrad^t, p^{t+1}}}{L_t \norm{p^{t+1}}^2}$\label{marker-aecg-unbounded-lt}\;
        \IF{$f(w^t + \eta_t p^{t+1}) - f(w^t) \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{2L_t\norm{p^{t+1}}^2}$}
        \STATE set $w^{t+1} \leftarrow w^t + \eta_t p^{t+1}$\;
        \ELSE
        \STATE set $L_t \leftarrow 2 \cdot L_t$\;
        \STATE \textbf{goto} line \ref{marker-aecg-unbounded-lt}\;
        \ENDIF
        \ENDFOR
    \end{algorithmic}\label{alg:aecg-unbounded-lt}
\end{algorithm}
%
\begin{theorem} Let $\eta_t = -\frac{\vprod{\errgrad^t, p^{t+1}}}
        {L_t \norm{p^{t+1}}^2}$ be the step-size of the Algorithm
    \ref{alg:aecg-unbounded-lt} then the following inequality holds:

    \begin{equation}
        f^{t+1} - f^t \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{2 L_t \norm{p^{t+1}}^2}
    \end{equation}
\end{theorem}
%
\begin{proof}
    From the descent lemma \eqref{lemma:descent-lemma} and
    the properties of \textbf{EO} follows that:

    \begin{equation}\label{eq:adaptive-erroneous-gradient-inexact-gradient-unbounded-lt}
        f^{t+1} - f^* \leqslant f^t - f^* + \eta_t \vprod{\errgrad^t, p^{t+1}} +
        \frac{L_t \eta_t^2}{2} \norm{p^{t+1}}^2
    \end{equation}

    \begin{equation}\label{eq:adaptive-erroneous-gradient-inexact-gradient-unbounded-lt-cont}
        f^{t+1} - f^t \leqslant \eta_t \vprod{\errgrad^t, p^{t+1}} +
        \frac{L_t \eta_t^2}{2} \norm{p^{t+1}}^2
    \end{equation}

    Plugging the step-size regime
    \eqref{eq:step-size-l-smoothness-constant-dependent} into
    \eqref{eq:adaptive-erroneous-gradient-inexact-gradient-unbounded-lt-cont}
    we have:

    \begin{equation}
        \begin{split}
            f^{t+1} - f^t \leqslant
            -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{L_t\norm{p^{t+1}}^2} +
            \frac{\vprod{\errgrad^t, p^{t+1}}^2}{2L_t\norm{p^{t+1}}^2}
            \leqslant                             \\
            \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{L_t\norm{p^{t+1}}^2}
            \Bigl(1 - \frac{1}{2}\Bigr) \leqslant \\
            \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{2L_t\norm{p^{t+1}}^2}
            \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{2L_tR^2}
        \end{split}
    \end{equation}

\end{proof}
%
\subsection{Bounded $L_t$}
%
\begin{algorithm}[!ht]
    \caption{Adaptive Erroneous Conditional Gradient (AECG) with bounded $L_t$}
    \begin{algorithmic}[1]
        \REQUIRE $N > 0$ -- maximum iterations, $w^0 \in \mathcal{C}, L_0 > 0, \varepsilon \in [0, 1), M > 0, R > 0$.
        \STATE Set $w^t \leftarrow w^0$\;
        \STATE Set $L_{t-1} \leftarrow L_0$\;
        \FOR{$t=0,1,\ldots,N-1$}
        \STATE set $L_{t} \leftarrow \frac{L_{t-1}}{2}$\;
        \STATE retrieve $\hat{g}^t \leftarrow \mathcal{O}(\nabla f(w^t), \varepsilon)$\label{marker-aecg-bounded-lt}\;
        \STATE compute $p^{t+1} \leftarrow LMO(\hat{g}^t) - w^t$
        \STATE compute $\eta_t \leftarrow -\frac{\vprod{\errgrad^t, p^{t+1}}}{L_t \norm{p^{t+1}}^2}$\;
        \IF{$f^{t+1} - f^t \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}
                    (\frac{1}{2}\vprod{\errgrad^t, p^{t+1}} + \varepsilon M R)}{2 L_t \norm{p^{t+1}}^2}$}
        \STATE set $w^{t+1} \leftarrow w^t + \eta_t p^{t+1}$\;
        \ELSE
        \STATE set $L_t \leftarrow 2 \cdot L_t$\;
        \STATE \textbf{goto} line \ref{marker-aecg-bounded-lt}\;
        \ENDIF
        \ENDFOR
    \end{algorithmic}\label{alg:aecg-bounded-lt}
\end{algorithm}
%
\begin{theorem} Let $\eta_t = -\frac{\vprod{\errgrad^t, p^{t+1}}}
        {L_t \norm{p^{t+1}}^2}$ be the step-size of the Algorithm
    \ref{alg:aecg-bounded-lt} then the following inequality holds:

    \begin{equation}
        f^{t+1} - f^t \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}
            (\frac{1}{2}\vprod{\errgrad^t, p^{t+1}} + \varepsilon M R)}{2 L_t \norm{p^{t+1}}^2}
    \end{equation}
\end{theorem}
%
\begin{proof}
    From the descent lemma \eqref{lemma:descent-lemma} and
    the properties of \textbf{EO} follows that:

    \begin{equation}\label{eq:adaptive-erroneous-gradient-inexact-gradient-bounded-lt}
        f^{t+1} - f^* \leqslant f^t - f^* + \eta_t \vprod{\errgrad^t, p^{t+1}} +
        \frac{L_t \eta_t^2}{2} \norm{p^{t+1}}^2 + \eta_t \varepsilon M R
    \end{equation}

    \begin{equation}\label{eq:adaptive-erroneous-gradient-inexact-gradient-bounded-lt-cont}
        f^{t+1} - f^t \leqslant \eta_t \vprod{\errgrad^t, p^{t+1}} +
        \frac{L_t \eta_t^2}{2} \norm{p^{t+1}}^2  + \eta_t \varepsilon M R
    \end{equation}

    Plugging the step-size regime
    \eqref{eq:step-size-l-smoothness-constant-dependent} into
    \eqref{eq:adaptive-erroneous-gradient-inexact-gradient-bounded-lt-cont}
    we have:

    \begin{equation}
        \begin{split}
            f^{t+1} - f^t \leqslant
            -\frac{\vprod{\errgrad^t, p^{t+1}}^2}{L_t\norm{p^{t+1}}^2} +
            \frac{\vprod{\errgrad^t, p^{t+1}}^2}{2L_t\norm{p^{t+1}}^2}
            -\frac{\vprod{\errgrad^t, p^{t+1}}}{L_t\norm{p^{t+1}}^2}
            \varepsilon M R \leqslant \\
            \leqslant -\frac{\vprod{\errgrad^t, p^{t+1}}(\frac{1}{2}\vprod{\errgrad^t, p^{t+1}}
                + \varepsilon M R)}{L_t\norm{p^{t+1}}^2}
        \end{split}
    \end{equation}

\end{proof}
%
\section{Computational experiments}
%
\section{Conclusion}
%
\end{document}
